#!/usr/bin/env python3
"""
A script to find and group duplicate files.

For example, if you have 6 files {00..06}.png where 00.png and 05.png
have the exact same file contents, running this program on them will
produce:

> 00.png:05.png
> 01.png
> 02.png
> 03.png
> 04.png
> 06.png

This program tries to be smart about how it compares files, first grouping
by hash and then comparing by either file inode or file contents to assert
whether two files are duplicates of each other.

For example, to get the list of all duplicate files excluding the first none
duplicate file, you can run: fdupes - -o | cut -d ':' -f 2- | tr ':' '\n'

# fdupes
This script isn't a reimplementation of [[https://linux.die.net/man/1/fdupes][fdupes]]. It lacks any mechanism
for file traversal or symlink following etc. Rather than building such
functionality into this script, I recommend using find or fd.

For example: find ~/multifolder/ -type f | fdupes -

If you prefer the readable output format of the existing fdupes utility
you can set flags to get a similair affect: fdupes -r '\n\n' -s '\n'.

The following command produces exactly the same output as the equivelant
fdupes command: find ~/multifolder/ -type f | fdupes - -r '\n\n' -s '\n' -o
"""

import os
import logging

import filecmp
import hashlib
import itertools
import collections

def is_duplicate(f1, f2):
    """Assert whether two files are equal.
    assumes f1!=f2
    """
    # perform a shallow check first and a full check only
    # when the shallow check fails.
    # ( get_hash(f1) == get_hash(f2) and
    return filecmp.cmp(f1, f2, shallow=True) or \
          filecmp.cmp(f1, f2, shallow=False)

_get_hash_type = collections.namedtuple('HashType', ['hash', 'time'])
def get_hash(fp, buf_size=8192, mem={}): # pylint: disable=W0102
    """Get the hash for the file at `fp'.
    Employs a caching mechanism to avoid recalculating the
    hash for a file you've already supplied. The cache is
    invalidated if the file pointed to by `fp' has been
    modified since we last calculated a hash value.
    """
    logging.debug('Getting hash for file: %s', fp)
    mtime = os.stat(fp).st_mtime
    if fp not in mem or mtime != mem[fp].time:
        logging.debug('Hash not in memory, generating from file path')
        m = hashlib.md5()
        with open(fp, 'rb') as fd:
            data = fd.read(buf_size)
            while data:
                m.update(data)
                data = fd.read(buf_size)
            fp_hash = m.hexdigest()
        mem[fp] = _get_hash_type(fp_hash, mtime)

    return mem[fp].hash

def group_equals(iterable, key=lambda a, b: a == b):
    """Group equivalent elements together in a collection.
    Uses key to determine whether two elements are equal.

    This implementation assumes transativity in key func,
    I.E. for arr = [1,1], arr[0] == arr[1] -> arr[1] == arr[0].
    """
    groups = []
    for it in iterable:
        for group in groups:
            if key(group[0], it):
                group.append(it)
                break
        else:
            groups.append([it])
    return groups

def gen_dups(paths):
    """Enumerate entries in `paths` grouped by duplicates.
    """
    # Turns out itertools.groupby only groups sequentially matching items
    # like `uniq`. We're gonna have to find and keep track of all the file
    # hashes in advance until I get a chance to implement a lazy groupby.
    hashes = {}
    for fp in paths:
        try:
            hashes[fp] = get_hash(fp)
        except: # pylint: disable=W0702
            logging.exception('Failed to get hash for file at path: %s', fp)
    paths = sorted(hashes.keys(), key=hashes.__getitem__)

    for fhash, path_gen in itertools.groupby(paths, key=hashes.__getitem__):
        # we have a list of files that have the same hash, now we need
        # to group files that're identicle together. There's no guarantee
        # that the same hash leads to the same file.
        for eq_paths in group_equals(path_gen, key=is_duplicate):
            yield fhash, eq_paths


if __name__ == '__main__':
    import sys
    import codecs
    import argparse
    from mohkale.pylog.config import use_config as use_logging_config

    parser = argparse.ArgumentParser(
        formatter_class=argparse.RawDescriptionHelpFormatter,
        description="Compare and group duplicate files.")

    read_group = parser.add_argument_group('Read Paths')
    read_group.add_argument('path', nargs='*', help='Specify paths to check.')
    read_group.add_argument('-', '--stdin', action='store_true',
                            help='Read paths to check from stdin.')

    output_group = parser.add_argument_group('Output',
                                             description="""
Configure how this program outputs results. By default it joins
each equal file with a field seperator (:) and seperates unequal
files with row seperator (\\n).
                                                """.strip())
    output_group.add_argument('-s', '--field-seperator',
                              default=':', metavar='SEP',
                              help='Seperator for identicle files in the same column')
    output_group.add_argument('-r', '--row-seperator',
                              default='\n', metavar='SEP',
                              help='Seperator for identicle files in the same column')

    # dup_pred_group = output_group.add_argument_group()
    dup_pred_group = output_group.add_mutually_exclusive_group()
    dup_pred_group.add_argument('-o', '--only',
                                action='store_true', dest='only_duplicates',
                                help='Only print files with at least one duplicate.')
    dup_pred_group.add_argument('-O', '--except',
                                action='store_true', dest='no_duplicates',
                                help='Only print files that have no duplicates.')

    parser.add_argument('-l', '--log-level', metavar='LEVEL',
                        type=lambda X: getattr(logging, X.upper()),
                        help='Level of logging output.')

    args  = parser.parse_args()
    vargs = vars(args)

    if args.stdin:
        args.path = itertools.chain(args.path, map(lambda x: x.rstrip(os.linesep), sys.stdin))

    args.field_seperator = codecs.decode(args.field_seperator, 'unicode_escape')
    args.row_seperator = codecs.decode(args.row_seperator, 'unicode_escape')

    use_logging_config('fdupes', level=vargs.pop('log_level'))

    for _, fs in gen_dups(args.path):
        if args.only_duplicates and len(fs) == 1 or\
           args.no_duplicates and len(fs) != 1:
            continue
        print(args.field_seperator.join(fs), end=args.row_seperator)
